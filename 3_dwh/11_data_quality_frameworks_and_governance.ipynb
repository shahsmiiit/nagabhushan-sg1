{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65541892",
   "metadata": {},
   "source": [
    "# Data Quality Frameworks & Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf243d",
   "metadata": {},
   "source": [
    "## What is a Data Quality Framework?\n",
    "\n",
    "A **Data Quality (DQ) Framework** is a structured approach for defining, running, and monitoring data quality checks across a data pipeline. \n",
    "It standardizes *what* to check (rules), *when* to check (ingestion vs transformation vs consumption), and *what to do* when data fails. \n",
    "The goal is to make data quality repeatable and measurable, instead of relying on ad-hoc manual checks. \n",
    "In modern platforms, a DQ framework is often embedded into pipelines so quality is validated continuously as data moves from bronze to silver to gold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0ec7a",
   "metadata": {},
   "source": [
    "## Rules-based DQ Checks\n",
    "\n",
    "**Rules-based checks** validate data against explicit business or technical rules, such as “customer_id cannot be null” or “order_amount must be >= 0”. \n",
    "These rules typically map to quality dimensions like completeness, validity, accuracy, and consistency. \n",
    "A good rule is specific, testable, and tied to a dataset and column(s), with a clear pass/fail outcome. \n",
    "Rules-based checks are the foundation for automation because they can run at scale on every batch or streaming micro-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28604d13",
   "metadata": {},
   "source": [
    "## Thresholds and Scorecards\n",
    "\n",
    "A **threshold** defines how much bad data is acceptable before the pipeline is considered unhealthy (for example, “null rate must be < 0.1%”). \n",
    "Thresholds are practical because real-world data can have occasional issues, and you may want to alert instead of blocking everything. \n",
    "A **scorecard** aggregates multiple check results into a simple view (by dataset, domain, or pipeline run) so teams can track quality over time. \n",
    "Scorecards help compare quality across releases and highlight whether data quality is improving, stable, or degrading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a2f65",
   "metadata": {},
   "source": [
    "## Exception Handling\n",
    "\n",
    "**Exception handling** defines what action the pipeline takes when a check fails. \n",
    "Common patterns include: **fail the pipeline** (hard stop), **quarantine** bad records to a separate table, **drop** invalid rows, or **continue with alerts**. \n",
    "The right choice depends on business risk: critical compliance datasets often require hard stops, while low-risk datasets may allow quarantine. \n",
    "Effective exception handling also records *why* the failure happened and *which rows* were affected, enabling faster root-cause analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f9987",
   "metadata": {},
   "source": [
    "## Audit, Balance & Controls (ABC)\n",
    "\n",
    "**Audit, Balance & Controls (ABC)** refers to operational checks that prove pipeline correctness beyond row-level validation. \n",
    "**Audit** typically tracks run metadata: when data arrived, which source file/version was used, counts processed, and success/failure status. \n",
    "**Balance** checks ensure data reconciliation across stages (for example, source count vs bronze count vs silver count, or totals matching across transformations). \n",
    "**Controls** define governance and enforcement mechanisms: approvals, access controls, lineage, retention rules, and documented sign-offs for critical datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247b4e5",
   "metadata": {},
   "source": [
    "## Quality Checks (Null / Duplicate / Type Checks)\n",
    "\n",
    "Quality checks validate that data is usable and trustworthy before it is consumed by downstream tables, dashboards, or models. \n",
    "**Null checks** ensure mandatory fields are populated (for example, `customer_id` or `order_id` cannot be missing). \n",
    "**Duplicate checks** detect repeated records where uniqueness is expected (for example, duplicate primary keys or duplicate transactions). \n",
    "**Type checks** confirm columns match expected data types and formats (for example, dates are real dates, numeric fields do not contain text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5cb6a3",
   "metadata": {},
   "source": [
    "## Null Checks\n",
    "\n",
    "A null check evaluates whether required columns contain missing values and whether the null rate is within an acceptable threshold. \n",
    "Nulls in key columns can break joins, cause record loss, and create inconsistent aggregates across layers. \n",
    "A common practice is to define mandatory columns per dataset and validate them during ingestion (bronze) and after transformations (silver). \n",
    "When nulls are found, pipelines may fail, quarantine bad rows, or apply controlled defaulting based on business policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9353c4a",
   "metadata": {},
   "source": [
    "## Duplicate Checks\n",
    "\n",
    "Duplicate checks ensure the same logical entity is not stored multiple times when it should be unique. \n",
    "Duplicates often appear due to reprocessing, late-arriving files, upstream retries, or missing deduplication logic in ingestion. \n",
    "A practical approach is to define the **uniqueness key** (single or composite) and check whether it repeats within a load window. \n",
    "If duplicates are allowed by design (for example, event logs), the rule must be clarified so it does not create false alarms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efaa9d0",
   "metadata": {},
   "source": [
    "## Type Checks\n",
    "\n",
    "Type checks verify that each field conforms to the schema expectations (for example, integers remain integers and timestamps remain timestamps). \n",
    "Type drift is common when raw sources change formats or when ingestion lands all fields as strings. \n",
    "Incorrect types can cause silent calculation errors, incorrect comparisons, and failed downstream writes. \n",
    "Type checks typically include schema validation, parse success rates, and explicit casting with error handling rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785027f",
   "metadata": {},
   "source": [
    "## Lineage Basics\n",
    "\n",
    "**Data lineage** describes where data came from, how it moved, and what transformations were applied along the way. \n",
    "At a minimum, lineage should answer: which source system/file produced this dataset, and which tables or jobs contributed to it. \n",
    "Lineage improves trust because analysts can trace unexpected results back to upstream inputs and transformations. \n",
    "In governed environments, lineage is also required for audits, impact analysis, and change management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db78f3",
   "metadata": {},
   "source": [
    "## Business Rules\n",
    "\n",
    "**Business rules** are domain-specific validations that reflect real operational meaning, not just technical correctness. \n",
    "Examples include “order_status must be one of the approved statuses” or “refund_amount cannot exceed order_amount.” \n",
    "Business rules must be documented with clear ownership because they represent business policy and can change over time. \n",
    "Embedding business rules into silver/gold pipelines ensures curated datasets represent consistent and approved definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961f9c5",
   "metadata": {},
   "source": [
    "## Intro to PII and Masking\n",
    "\n",
    "**PII (Personally Identifiable Information)** is data that can identify an individual, directly or indirectly (such as name, phone, email, address, government ID). \n",
    "PII must be protected because misuse can lead to privacy violations, regulatory penalties, and loss of customer trust. \n",
    "**Masking** reduces exposure by hiding sensitive values while keeping the dataset usable for analysis and testing. \n",
    "Common masking approaches include redaction (showing partial values), tokenization, hashing, and role-based views that reveal full data only to authorized users."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
