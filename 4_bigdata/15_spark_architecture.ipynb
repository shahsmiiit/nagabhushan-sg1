{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ce4005",
   "metadata": {},
   "source": [
    "# Apache Spark Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2779cae",
   "metadata": {},
   "source": [
    "\n",
    "## Spark Cluster Architecture\n",
    "\n",
    "Apache Spark is a **distributed compute engine**. A Spark application runs across a **cluster** (a group of machines), even if you are using a managed platform like Databricks or EMR.\n",
    "\n",
    "### The big picture\n",
    "\n",
    "- **Driver**: the brain of your Spark application (plans and coordinates work)\n",
    "- **Executors**: the workers that run tasks and process partitions\n",
    "- **Cluster Manager**: allocates resources (YARN / Kubernetes / Standalone)\n",
    "- **Storage**: where input/output data lives (HDFS, S3, ADLS, GCS, DBFS, etc.)\n",
    "\n",
    "\n",
    "![Spark Cluster Architecture](../images/spark_cluster_architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa03ef",
   "metadata": {},
   "source": [
    "\n",
    "### What each component does\n",
    "\n",
    "#### Driver\n",
    "- Creates the SparkSession / SparkContext\n",
    "- Builds the logical plan (what to do)\n",
    "- Builds the physical execution plan (how to do it)\n",
    "- Splits the work into **jobs → stages → tasks**\n",
    "- Sends tasks to executors and tracks progress\n",
    "\n",
    "#### Executors\n",
    "- JVM processes on worker nodes\n",
    "- Run **tasks** on **partitions**\n",
    "- Store cached data (if you use cache/persist)\n",
    "- Write shuffle files during wide transformations\n",
    "\n",
    "#### Cluster manager\n",
    "- Controls resource allocation (CPU/memory) for executors\n",
    "- Examples: **YARN**, **Kubernetes**, **Standalone**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54aaf90",
   "metadata": {},
   "source": [
    "\n",
    "## How a Spark Application Runs \n",
    "\n",
    "Spark is **lazy**: transformations build a plan, and Spark actually runs when an **action** happens.\n",
    "\n",
    "### Typical workflow\n",
    "\n",
    "1. **Driver starts** and creates SparkSession/SparkContext  \n",
    "2. You define transformations (map/filter/select/withColumn/...)  \n",
    "3. Spark builds a **DAG** (Directed Acyclic Graph) of transformations  \n",
    "4. When an **action** happens (count/collect/write/show/...), Spark creates a **job**  \n",
    "5. Job is split into **stages** (based on shuffle boundaries)  \n",
    "6. Each stage is split into **tasks** (1 task per partition)  \n",
    "7. Tasks run on executors; shuffle may occur between stages  \n",
    "8. Results are returned to driver or written to storage\n",
    "\n",
    "### Image placeholder: DAG → stages → tasks\n",
    "![DAG, Stages, Tasks](../images/spark_dag_stages_tasks.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd022f2a",
   "metadata": {},
   "source": [
    "\n",
    "### Narrow vs Wide transformations (why stages form)\n",
    "\n",
    "- **Narrow transformation**: each output partition depends on **one** input partition  \n",
    "  Examples: `map`, `filter`, `flatMap`, `select`, `withColumn`  \n",
    "  ✅ Usually stays within the same stage\n",
    "\n",
    "- **Wide transformation**: output partition depends on **many** input partitions  \n",
    "  Examples: `groupByKey`, `reduceByKey`, `join`, `groupBy`, `distinct`  \n",
    "  ⚠️ Requires **shuffle** and creates a new stage\n",
    "\n",
    "Spark docs explain shuffle as the mechanism that re-distributes data across partitions and executors, often involving disk + network IO.\n",
    "\n",
    "### Image placeholder: Shuffle\n",
    "![Shuffle](../images/spark_shuffle.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6d539",
   "metadata": {},
   "source": [
    "\n",
    "## Key Terminologies \n",
    "\n",
    "- **Application**: one Spark program run (one driver + many executors)\n",
    "- **Job**: created by an **action** (e.g., `count()`, `collect()`, `write`, `show()`)\n",
    "- **Stage**: a set of tasks that can run without shuffle (bounded by shuffle)\n",
    "- **Task**: smallest unit of work; processes **one partition**\n",
    "- **Partition**: a slice of data; the unit of parallelism\n",
    "- **DAG**: graph of transformations; Spark schedules it efficiently\n",
    "- **Executor**: process on workers that runs tasks\n",
    "- **Driver**: coordinator; schedules stages/tasks; holds SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8ece7",
   "metadata": {},
   "source": [
    "\n",
    "## SparkSession\n",
    "\n",
    "**SparkSession** is the modern entry point (Spark 2.0+). It unifies:\n",
    "- DataFrame / Dataset APIs\n",
    "- Spark SQL\n",
    "- Streaming and more\n",
    "\n",
    "In Databricks notebooks, a `spark` session is usually already available.\n",
    "\n",
    "### Image placeholder: SparkSession as unified entry point\n",
    "![SparkSession](../images/spark_session_unified_entry.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b91072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession is usually available in Databricks as `spark`\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38e507",
   "metadata": {},
   "source": [
    "\n",
    "## SparkContext\n",
    "\n",
    "**SparkContext** represents the connection to the Spark cluster and is the classic entry point for RDD operations.\n",
    "\n",
    "In modern Spark, you typically access it via:\n",
    "\n",
    "`spark.sparkContext`\n",
    "\n",
    "SparkContext still matters when:\n",
    "- working with RDDs\n",
    "- low-level operations (broadcast variables, accumulators, etc.)\n",
    "- legacy codebases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de37892c",
   "metadata": {},
   "source": [
    "\n",
    "## SQLContext \n",
    "\n",
    "Before Spark 2.0, structured operations were done via SQLContext / HiveContext.\n",
    "From Spark 2.0 onward, **SparkSession replaces SQLContext** for most use cases.\n",
    "\n",
    "You may still see SQLContext in older tutorials or older Spark code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1b523",
   "metadata": {},
   "source": [
    "\n",
    "## Spark Execution Methods\n",
    "\n",
    "### Interactive execution (explore + learn)\n",
    "- `spark-shell` (Scala REPL)\n",
    "- `pyspark` shell\n",
    "- Notebooks (Databricks / Jupyter / Zeppelin)\n",
    "\n",
    "✅ Best for learning, debugging, exploratory analysis.\n",
    "\n",
    "### Job submission (production runs)\n",
    "- `spark-submit` (most common)\n",
    "- REST APIs (platform-dependent)\n",
    "\n",
    "✅ Best for scheduled batch pipelines and production workloads.\n",
    "\n",
    "Spark docs for application submission: https://spark.apache.org/docs/latest/submitting-applications.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf227ee",
   "metadata": {},
   "source": [
    "\n",
    "### Deploy modes (client vs cluster)\n",
    "\n",
    "Where does the **driver** run?\n",
    "\n",
    "- **client mode**: driver runs where you run `spark-submit`\n",
    "- **cluster mode**: driver runs inside the cluster (e.g., YARN ApplicationMaster / a Kubernetes driver pod)\n",
    "\n",
    "Spark’s YARN docs describe these modes: https://spark.apache.org/docs/latest/running-on-yarn.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b8fe9",
   "metadata": {},
   "source": [
    "\n",
    "## RDD example (stages + shuffle)\n",
    "\n",
    "This example demonstrates a classic **word count** flow:\n",
    "\n",
    "- `flatMap` → `map` are **narrow** transformations (no shuffle)\n",
    "- `reduceByKey` is a **wide** transformation (shuffle) → creates a new stage\n",
    "- `collect` is an **action** → triggers execution\n",
    "\n",
    "### Image placeholder: wordcount DAG\n",
    "![WordCount DAG](../images/spark_dag_stages_tasks.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD word count (small demo)\n",
    "text = [\"hello spark\", \"hello lakehouse\", \"hello world\", \"spark is fast\"]\n",
    "rdd = sc.parallelize(text, 2)\n",
    "\n",
    "words = rdd.flatMap(lambda line: line.split(\" \"))           # narrow\n",
    "pairs = words.map(lambda w: (w, 1))                         # narrow\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)              # wide (shuffle)\n",
    "\n",
    " counts.collect()                                   # action\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699247d",
   "metadata": {},
   "source": [
    "\n",
    "##  Spark Web UI (what to look for)\n",
    "\n",
    "Every Spark application exposes a Web UI (commonly on port 4040). It shows:\n",
    "- Jobs, Stages, Tasks\n",
    "- Executors\n",
    "- Storage (cached datasets)\n",
    "- SQL tab (for Spark SQL queries)\n",
    "- Environment and event timeline\n",
    "\n",
    "Official docs: https://spark.apache.org/docs/latest/monitoring.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e741c",
   "metadata": {},
   "source": [
    "\n",
    "## Official references (Spark docs)\n",
    "\n",
    "- Cluster Mode Overview: https://spark.apache.org/docs/latest/cluster-overview.html  \n",
    "- Submitting Applications (spark-submit): https://spark.apache.org/docs/latest/submitting-applications.html  \n",
    "- RDD Programming Guide (transformations, actions, shuffle): https://spark.apache.org/docs/latest/rdd-programming-guide.html  \n",
    "- Monitoring / Spark Web UI: https://spark.apache.org/docs/latest/monitoring.html  \n",
    "- SparkSession API (Python): https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html  \n",
    "- SparkContext API (Python): https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html  \n",
    "- SQLContext (legacy): https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SQLContext.html  \n",
    "- Running on YARN (deploy modes): https://spark.apache.org/docs/latest/running-on-yarn.html  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
