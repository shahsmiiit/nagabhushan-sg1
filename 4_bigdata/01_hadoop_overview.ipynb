{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa9574f",
   "metadata": {},
   "source": [
    "# Hadoop Overview \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a322a",
   "metadata": {},
   "source": [
    "## A simple story: why distributed computing exists\n",
    "\n",
    "Large-scale problems (like search, logs, media, clickstreams) become too big for one machine.\n",
    "\n",
    "**Distributed computing** means:\n",
    "- Put **many small and inexpensive computers together**\n",
    "- Each computer is a **node**\n",
    "- Many nodes together form a **cluster**\n",
    "- The cluster behaves like a single system to solve a bigger problem\n",
    "\n",
    "### Why it is powerful\n",
    "A cluster can scale performance by adding more nodes:\n",
    "- Need more compute? Add more nodes.\n",
    "- Need more storage? Add more nodes.\n",
    "- Failures are expected, so the system must keep working even if a node goes down (fault tolerance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6f2df",
   "metadata": {},
   "source": [
    "![Hadoop](../images/hdfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95504cb",
   "metadata": {},
   "source": [
    "## The problem Hadoop solves\n",
    "\n",
    "Distributed systems are hard because you must handle:\n",
    "1. **Resource and memory management** across nodes\n",
    "2. **Coordination and scheduling** of work across nodes\n",
    "3. **Fault tolerance** (node failures should not bring the system down)\n",
    "\n",
    "Before modern frameworks, programmers had to handle these complexities themselves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71bb8df",
   "metadata": {},
   "source": [
    "## The Google papers that inspired Hadoop\n",
    "\n",
    "Between 2003 and 2006, Google published three influential papers that shaped modern big data systems:\n",
    "\n",
    "1. **Google File System (GFS)** → architecture for distributed storage  \n",
    "2. **MapReduce** → architecture for distributed processing  \n",
    "3. **BigTable** → architecture for distributed database management  \n",
    "\n",
    "Open-source systems were built around these ideas:\n",
    "- **HDFS** (inspired by GFS) — storage\n",
    "- **Hadoop MapReduce** (inspired by MapReduce) — processing\n",
    "- **HBase** (inspired by BigTable) — database-style access on top of HDFS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54e15d",
   "metadata": {},
   "source": [
    "## Hadoop: what it is\n",
    "\n",
    "**Hadoop** is a distributed computing framework (Apache) written in Java.\n",
    "\n",
    "In the slides, Hadoop is presented as two core parts:\n",
    "\n",
    "- **HDFS**: a distributed file system to store data across multiple machines and disks\n",
    "- **MapReduce**: a framework to process data across multiple servers\n",
    "\n",
    "In the same ecosystem view, the slides also mention:\n",
    "- **HBase**: a database-style system built for big data use cases (inspired by BigTable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e80ace",
   "metadata": {},
   "source": [
    "## HDFS (Hadoop Distributed File System)\n",
    "\n",
    "HDFS stores large files by distributing them across machines.\n",
    "\n",
    "### Key components\n",
    "- **NameNode (master)**  \n",
    "  Stores:\n",
    "  - Directory structure (folders, filenames)\n",
    "  - Metadata (file → blocks, permissions, etc.)\n",
    "  - **Block locations** (which DataNodes hold which blocks)\n",
    "\n",
    "- **DataNodes (workers)**  \n",
    "  Store the actual data blocks on disk.\n",
    "\n",
    "### Key idea: blocks\n",
    "When you store a large file in HDFS:\n",
    "1. The file is split into fixed-size **blocks** (in these slides: **128 MB** blocks)\n",
    "2. Blocks are distributed across DataNodes\n",
    "3. NameNode tracks where every block is stored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2020d4",
   "metadata": {},
   "source": [
    "![Hadoop](../images/hadoop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b532fa",
   "metadata": {},
   "source": [
    "## Reading a file in HDFS (high-level)\n",
    "\n",
    "When a client reads a file:\n",
    "1. The client asks the **NameNode** for metadata and block locations.\n",
    "2. The client reads the blocks directly from the relevant **DataNodes**.\n",
    "\n",
    "The NameNode does **not** serve the actual file data — it serves metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c21d3bd",
   "metadata": {},
   "source": [
    "## Fault tolerance in HDFS: replication\n",
    "\n",
    "In distributed storage, failures happen:\n",
    "- A block can be corrupted\n",
    "- A DataNode can crash\n",
    "\n",
    "HDFS handles this with a **replication factor**:\n",
    "- Each block is replicated\n",
    "- Replicas are stored on different DataNodes\n",
    "- The NameNode stores the locations of all replicas\n",
    "\n",
    "Result:\n",
    "- If a DataNode fails, another replica can be used.\n",
    "- Data availability improves significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0980a",
   "metadata": {},
   "source": [
    "## MapReduce: how Hadoop processes data\n",
    "\n",
    "MapReduce is a way to **parallelize** a data processing task.\n",
    "\n",
    "### The two phases\n",
    "1. **Map phase**\n",
    "   - Process data **where it is stored** (on the DataNodes that hold the blocks)\n",
    "   - Produce intermediate results\n",
    "\n",
    "2. **Reduce phase**\n",
    "   - Collect intermediate results and **combine/aggregate** them\n",
    "   - Produce the final output\n",
    "\n",
    "### What the programmer does vs what Hadoop handles\n",
    "- Programmer: writes the logic for **map** and **reduce**\n",
    "- Hadoop: handles distribution, scheduling, failures, and moving intermediate data as required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc09dfd",
   "metadata": {},
   "source": [
    "![Hadoop](../images/mapreduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3111a",
   "metadata": {},
   "source": [
    "## Hadoop Components\n",
    "\n",
    "\n",
    "- **HDFS** → distributed storage (GFS-inspired)\n",
    "- **Hadoop MapReduce** → distributed batch processing (MapReduce-inspired)\n",
    "- **HBase** → database-style access layer (BigTable-inspired)\n",
    "\n",
    "Together, this enables:\n",
    "1. Store large datasets across a cluster\n",
    "2. Process them in parallel across that cluster\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
