{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670e8fba",
   "metadata": {},
   "source": [
    "# EMR SSH Access + HDFS Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a807dc",
   "metadata": {},
   "source": [
    "## Download the PEM key\n",
    "\n",
    "Download **`emr_training1.pem`** from the shared folder and save it locally.\n",
    "\n",
    "### Recommended location\n",
    "- **macOS/Linux**: `~/Downloads/emr_training1.pem`\n",
    "\n",
    "You will use this file to authenticate your SSH connection to the EMR node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914997ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download emr_training1.pem from the shared folder (manual step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366d98c",
   "metadata": {},
   "source": [
    "## Secure the PEM file (permissions)\n",
    "\n",
    "SSH requires the private key to be readable only by you.\n",
    "\n",
    "On **macOS/Linux**, use `chmod 600`:\n",
    "\n",
    "- `600` means: **read/write for owner only**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chmod 600 emr_training1.pem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450e187",
   "metadata": {},
   "source": [
    "## Connect to the EMR primary node using SSH\n",
    "\n",
    "Connect as the `hadoop` user to the EMR primary node:\n",
    "\n",
    "- Make sure the PEM path is correct for your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066fa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh -i emr_training1.pem hadoop@18.142.114.243"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0eb1b",
   "metadata": {},
   "source": [
    "## Verify Hadoop installation\n",
    "\n",
    "After logging in, confirm Hadoop is installed and available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f8614",
   "metadata": {},
   "source": [
    "## HDFS basics: list files\n",
    "\n",
    "`hadoop fs` is the CLI for **HDFS** operations.\n",
    "\n",
    "- `-ls` lists files and directories in HDFS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1723efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ebe58d",
   "metadata": {},
   "source": [
    "## Create your own HDFS folder\n",
    "\n",
    "Create a personal folder under `/user/hadoop/`.\n",
    "\n",
    "Replace `<your_name>` with your name (no spaces recommended).\n",
    "Example: `/user/hadoop/nagabhushan`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af89831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -mkdir /user/hadoop/<your_name>\n",
    "\n",
    "hadoop fs -mkdir /user/hadoop/<your_name>/stocks_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38e7f6",
   "metadata": {},
   "source": [
    "## Upload a single CSV file to HDFS\n",
    "\n",
    "Upload `nyse_sample_data.csv` to `/user/hadoop/`.\n",
    "\n",
    "- `-put` copies from **local filesystem** â†’ **HDFS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " hadoop fs -put nyse_sample_data.csv /user/hadoop/<your_name>/stocks_data/\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbdbe3",
   "metadata": {},
   "source": [
    "## Read the uploaded file using `cat` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /user/hadoop/<your_name>/stocks_data/nyse_sample_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071fa51b",
   "metadata": {},
   "source": [
    "## Upload the `retail_db/` folder to your HDFS folder\n",
    "\n",
    "This copies the entire `retail_db/` directory (including subfolders) into your HDFS location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be15ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -put retail_db/ /user/hadoop/<your_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b03890",
   "metadata": {},
   "source": [
    "## Preview file contents from HDFS\n",
    "\n",
    "### Read the full file (`cat`)\n",
    "Use `-cat` to print the entire file to the terminal (best for small files).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2aa83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -cat /user/hadoop/<your_name>/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67ee87",
   "metadata": {},
   "source": [
    "### View last lines (`tail`)\n",
    "Use `-tail` to view the last part of a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -tail /user/hadoop/<your_name>/retail_db/customers/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6b1d2",
   "metadata": {},
   "source": [
    "### View first lines (`head`)\n",
    "Use `-head` to view the first part of a file (useful for header/schema preview).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f56f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -head /user/hadoop/<your_name>/retail_db/order_items/part-00000"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
